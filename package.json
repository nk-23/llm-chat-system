{
  "name": "llm-chat-backend",
  "version": "1.0.0",
  "description": "Backend server for LLM Chat System with multiple AI providers",
  "main": "app.js",
  "scripts": {
    "start": "node app.js",
    "dev": "nodemon app.js",
    "test": "jest",
    "lint": "eslint ."
  },
  "dependencies": {
    "express": "^4.18.2",
    "body-parser": "^1.20.2",
    "dotenv": "^16.3.1",
    "node-fetch": "^2.6.7",
    "socket.io": "^4.7.2",
    "cors": "^2.8.5",
    "helmet": "^7.0.0",
    "express-rate-limit": "^6.10.0",
    "compression": "^1.7.4",
    "morgan": "^1.10.0"
  },
  "devDependencies": {
    "nodemon": "^3.0.1",
    "jest": "^29.6.2",
    "eslint": "^8.47.0",
    "supertest": "^6.3.3"
  },
  "engines": {
    "node": ">=16.0.0"
  },
  "keywords": [
    "llm",
    "chat",
    "ai",
    "openai",
    "azure",
    "llama",
    "claude",
    "websocket",
    "express"
  ],
  "author": "LLM Chat Team",
  "license": "MIT"
}
